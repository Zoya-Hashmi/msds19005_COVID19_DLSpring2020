{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "msds19005_05_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoiBu9rdWEMY",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Chest XRAYS Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoMyVNA5KH1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1-491oqElItj4TOTApzBgkfVCNsNH-_HL\n",
        "!unzip /content/A_05_Part_02_Dataset.zip\n",
        "data_dir = '/content/content/A_05_Part_02_Dataset'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBjhtaqXWPOP",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx1JAjdvKdNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atInXXMLWTWT",
        "colab_type": "text"
      },
      "source": [
        "## DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehv_JoycLpvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.Resize(256),\n",
        "                                       transforms.CenterCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.4924, 0.4924, 0.4925],\n",
        "                                                            [0.2491, 0.2491, 0.2491])])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4924, 0.4924, 0.4925],\n",
        "                                                            [0.2491, 0.2491, 0.2491])])\n",
        "\n",
        "\n",
        "#pass transform here-in\n",
        "train_data = datasets.ImageFolder(data_dir + '/Train', transform=train_transforms)\n",
        "val_data = datasets.ImageFolder(data_dir + '/Validation', transform=test_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/Test', transform=test_transforms,)\n",
        "\n",
        "#data loaders\n",
        "batch_size =32\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Classes: \")\n",
        "class_names = train_data.classes\n",
        "print(class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su24V39sWXNg",
        "colab_type": "text"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGfPb6DLLuRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_model(model,freeze):\n",
        "    if model == 'res18':\n",
        "        res18 = models.resnet18(pretrained=True)\n",
        "        # print(res18.fc.in_features)\n",
        "        fc1_in = res18.fc.in_features\n",
        "        # print(fc1_in)\n",
        "        \n",
        "        if freeze == 'all':\n",
        "            for param in res18.parameters():\n",
        "                param.requires_grad=False\n",
        "        elif freeze == 'partial':\n",
        "            for param in list(res18.parameters())[:-17]:\n",
        "                param.requires_grad=False\n",
        "        elif freeze == 'none':\n",
        "            pass\n",
        "\n",
        "\n",
        "    elif model == 'vgg16':\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        fc1_in = vgg16.classifier[0].in_features\n",
        "\n",
        "        if freeze == 'all':\n",
        "            for param in vgg16.features.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif freeze == 'partial':\n",
        "            for param in list(vgg16.features.parameters())[:-6]:\n",
        "                param.requires_grad = False\n",
        "        elif freeze == 'none':\n",
        "            pass\n",
        "\n",
        "\n",
        "    fc1_out = 150 #5*10+100\n",
        "    fc2_in = fc1_out\n",
        "    fc2_out = 3\n",
        "    features = [nn.Linear(fc1_in,fc1_out,bias=True),nn.ReLU(inplace=True),nn.Dropout(p=0.5,inplace=False),nn.Linear(fc2_in,fc2_out,bias=True)]\n",
        "\n",
        "    if model=='res18':\n",
        "        res18.fc = nn.Sequential(*features)\n",
        "        return res18\n",
        "    if model == 'vgg16':\n",
        "        vgg16.classifier = nn.Sequential(*features)\n",
        "        return vgg16\n",
        "\n",
        "net = 'res18'\n",
        "freeze = 'none'\n",
        "model = initialize_model(net,freeze)\n",
        "# pretrained_weights = '/content/drive/My Drive/focal_trained/res18_focal_loss.pth'\n",
        "# model.load_state_dict(torch.load(pretrained_weights)['state_dict'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN8lEPemWaxz",
        "colab_type": "text"
      },
      "source": [
        "## Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcTn2y9KMcWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class FocalLoss2d(nn.modules.loss._WeightedLoss):\n",
        "\n",
        "    def __init__(self, gamma=2,alpha=torch.Tensor([0.8764,0.0438,0.0797])):\n",
        "        super(FocalLoss2d, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, inpt, target):\n",
        "        \n",
        "        assert len(inpt.shape) == len(target.shape)\n",
        "        assert inpt.size(0) == target.size(0)\n",
        "        assert inpt.size(1) == target.size(1)\n",
        "        \n",
        "           \n",
        "        logpt = -torch.nn.BCEWithLogitsLoss(reduction='none')(inpt, target)\n",
        "\n",
        "        pt = torch.exp(logpt)\n",
        "        # print(pt.shape,logpt.shape,gamma.shape)\n",
        "\n",
        "        focal_loss = -( (1-pt).pow(self.gamma) ) * logpt\n",
        "        balanced_focal_loss = self.alpha*focal_loss\n",
        "        return torch.mean(balanced_focal_loss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPoE0KYBXd9I",
        "colab_type": "text"
      },
      "source": [
        "##Multilabel One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8m8gp7qMyQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # ['covid-19', 'normal', 'pneumonia']\n",
        "def one_hot_encode(label):\n",
        "  label = label.long()\n",
        "  # print(label.size())\n",
        "  l = torch.eye(3)\n",
        "  l[0] = l[0]+l[2]\n",
        "\n",
        "  return l[label]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_AOnznWkvP",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0s7wD_WMrPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "Epochs = 50\n",
        "lr = 1e-5\n",
        "\n",
        "\n",
        "\n",
        "loss_type = \"focal\"\n",
        "\n",
        "if loss_type == \"focal\":\n",
        "  \n",
        "  gamma = Variable(torch.Tensor([2]).to(device),requires_grad=True)\n",
        "  alpha = Variable(torch.Tensor([0.25,0.25,0.25]).to(device),requires_grad=True)\n",
        "\n",
        "  criterion = FocalLoss2d(gamma,alpha)\n",
        "\n",
        "\n",
        "  optimizer = optim.SGD([*model.parameters(),alpha,gamma], lr=lr, momentum=0.9)\n",
        "\n",
        "else:\n",
        "  criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "# criterion(torch.Tensor([[0.7,0.6,0.9],[0.3,0.8,0.4]]).to(device),torch.Tensor([[1,0,1],[1,1,0]]).to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7WAYSDFMvKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('/content/trained_models')\n",
        "path = '/content/trained_models'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPTIUZ2vM0eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "val_loss_min= 1e6\n",
        "val_acc_prev = 0.0\n",
        "\n",
        "epoch_lr = []\n",
        "epoch_tacc = []\n",
        "epoch_tloss = []\n",
        "epoch_vacc= []\n",
        "epoch_vloss = []\n",
        "\n",
        "\n",
        "for epoch in range(Epochs):  # loop over the dataset multiple times\n",
        "    val_accuracy =  0.0\n",
        "    train_accuracy = 0.0\n",
        "\n",
        "    epoch_lr.append(lr)\n",
        "    val_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(trainloader),position=0,leave=True)\n",
        "    for i,data in pbar:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)#, labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)               #----> forward pass\n",
        "        labels = one_hot_encode(labels).to(device)\n",
        "        loss = criterion(outputs,labels)\n",
        "        # loss.register_hook(lambda g: print(g))\n",
        "        \n",
        "        loss.backward()                     #----> backward pass\n",
        "        optimizer.step()                    #----> weights update\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        ##############\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        correct = ((outputs>0.5)==labels).float().sum()\n",
        "        #################\n",
        "\n",
        "        # correct = ((outputs>0.5)==labels).float().sum()\n",
        "        accuracy = correct*100/(outputs.shape[0]*outputs.shape[1])  #.mean()*100\n",
        "        train_accuracy+=accuracy\n",
        "\n",
        "        pbar.set_description(\n",
        "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.1f}%'.format(\n",
        "                epoch, i * len(inputs), len(trainloader.dataset),\n",
        "                100. * i / len(trainloader),\n",
        "                loss.data.item(),accuracy),refresh=False)\n",
        "            \n",
        "    print(\"\\nTraining Loss of Epoch \",epoch,\" is :\",running_loss)\n",
        "    \n",
        "    train_accuracy =  train_accuracy/len(trainloader)\n",
        "    print(\"Training Accuracy of Epoch \",epoch,\" is :\",train_accuracy.item(),\"\\n\\n\")\n",
        "    \n",
        "\n",
        "    epoch_tacc.append(train_accuracy)\n",
        "    epoch_tloss.append(running_loss)\n",
        "\n",
        "    model.eval()\n",
        "    pbar = tqdm(enumerate(valloader),position=0,leave=True)\n",
        "    for i,data in pbar:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = one_hot_encode(labels).to(device)\n",
        "        outputs = model(inputs)               #----> forward pass\n",
        "        loss = criterion(outputs, labels)   #----> compute loss\n",
        "    \n",
        "        val_loss += loss.item()\n",
        "        \n",
        "\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        correct = ((outputs>0.5)==labels).float().sum()\n",
        "        \n",
        "        # correct = ((outputs>0.5)==labels).float().sum()\n",
        "        accuracy = correct*100/(outputs.shape[0]*outputs.shape[1])\n",
        "        val_accuracy += accuracy\n",
        "        pbar.set_description(\n",
        "            'Validation Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.1f}%'.format(\n",
        "                epoch, i * len(inputs), len(valloader.dataset),\n",
        "                100. * i / len(valloader),\n",
        "                loss.data.item(),accuracy),refresh=False)\n",
        "        \n",
        "\n",
        "    print(\"\\nValidation Loss of Epoch \",epoch,\" is :\",val_loss)\n",
        "    val_accuracy = val_accuracy/len(valloader)\n",
        "    print(\"Validation Accuracy of Epoch \",epoch,\" is :\",val_accuracy.item(),\"\\n\\n\")\n",
        "\n",
        "    epoch_vloss.append(val_loss)\n",
        "    epoch_vacc.append(val_accuracy)\n",
        "\n",
        "    if val_loss < val_loss_min:\n",
        "        checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'valid_loss_min': val_loss,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "\n",
        "        val_loss_min = val_loss\n",
        "\n",
        "        torch.save(checkpoint, os.path.join(path,'vgg16_ft_{:.2f}_{:.2f}_{}.pth'.format(val_accuracy,val_loss,epoch)))\n",
        "\n",
        "    delta = abs(val_accuracy - val_acc_prev)\n",
        "    val_acc_prev = val_accuracy\n",
        "\n",
        "    if (delta < 0.4)and(lr<0.1):\n",
        "        lr = lr*10\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dozYFWkjWp5J",
        "colab_type": "text"
      },
      "source": [
        "## Loss and Accuracy Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e3MX3gqtPN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path= '/content/drive/My Drive/vgg_results'\n",
        "import os.path as osp\n",
        "\n",
        "Epochs=21\n",
        "## LOSS AND ACCURACY CURVES ##\n",
        "plt.figure()\n",
        "plt.plot(range(Epochs),np.array(epoch_tloss).reshape(Epochs),color='k',label='Train')\n",
        "plt.plot(range(Epochs),np.array(epoch_vloss).reshape(Epochs),color='b',label='Validation')\n",
        "\n",
        "\n",
        "plt.title('Loss Curves')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(1,50)\n",
        "plt.legend()\n",
        "\n",
        "# plt.savefig(osp.join(save_path,'Loss Curves.png'),bbox_inches='tight')\n",
        "################ ACCURACY##################\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(Epochs),epoch_tacc,color='k',label='Train')\n",
        "plt.plot(range(Epochs),epoch_vacc,color='b',label='Validation')\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(1,50)\n",
        "\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# plt.savefig(osp.join(save_path,'Accuracy Curves.png'),bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qnWN_K9WtmX",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZX9ktgDjfCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import itertools\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "correct = 0\n",
        "predicted = []\n",
        "gt = []\n",
        "cfm =[[[0,0],[0,0]],[[0,0],[0,0]],[[0,0],[0,0]]]\n",
        "with torch.no_grad():\n",
        "  for data in trainloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = one_hot_encode(labels).to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        \n",
        "        correct += ((outputs>0.5)==labels).float().sum()\n",
        "        \n",
        "        cfm += multilabel_confusion_matrix(torch.Tensor.cpu(labels), torch.Tensor.cpu(outputs>0.5))#,labels=[\"covid-19\",\"normal\",\"pneumonia\"]))\n",
        "\n",
        "print('Accuracy of the network on test images: %d %%' % (\n",
        "    100 * correct /(len(trainloader.dataset)*3 )))\n",
        "\n",
        "\n",
        "recall = cfm[0][1,1]/cfm[0][1,:].sum()\n",
        "precision = cfm[0][1,1]/cfm[0][:,1].sum()\n",
        "f1 = (2*precision*recall)/(precision+recall)\n",
        "print(\"F1 SCORE: \",f1)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "#     print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cfm[0], classes=['rest','covid-19'])\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cfm[1], classes=['rest','normal'])\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cfm[2], classes=['rest','pneumonia'])\n",
        "# plt.savefig(osp.join(save_path,'cfm_train.png'),bbox_inches='tight')\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EPSeJgiW8_6",
        "colab_type": "text"
      },
      "source": [
        "## Saving Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm63S8_LCHww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "for data in testloader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = one_hot_encode(labels).to(device)\n",
        "      \n",
        "      outputs = model(images)\n",
        "      outputs = torch.sigmoid(outputs)\n",
        "      \n",
        "\n",
        "      preds = torch.Tensor.cpu(outputs>0.5).int().numpy()\n",
        "      preds[:,[1,2]] = preds[:,[2,1]]\n",
        "      predictions.extend(preds)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(predictions)\n",
        "df[-1] = pd.Series([tup[0].split('/')[-1] for tup in testloader.dataset.samples])\n",
        "# df[-1] = pd.Series(['1','2','3','4','5','6'])\n",
        "df =  df[[-1,0,1,2]]\n",
        "df.to_csv('results.csv',header=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyfozdlr9eju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
